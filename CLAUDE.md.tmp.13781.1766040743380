# CLAUDE.md

AI assistant guidance for working with this repository.

## Project Overview

Production-ready CLI tool for validating the **(n-1)/2 conjecture**: the average discovery number of a node in randomized DFS on large symmetric regular graphs tends to (n-1)/2, where n is the number of nodes.

**Current Status:** Version 0.5.0 - Advanced visualization with histogram and layer analysis

## Quick Start

### Running the Application
```bash
# Interactive CLI
python3 run_analyzer.py

# Web GUI (new!)
python3 run_gui.py

# Install dependencies
pip install -r requirements.txt

# Run test scripts
python3 test_hypercube.py   # Test 5D hypercube
python3 test_petersen.py    # Test Petersen GP(5,2)
python3 test_gnp.py         # Test G(n,p) batch mode
python3 test_custom_vertex.py  # Test custom vertex pairs
```

## Project Architecture

### Modern Package Structure

**dfs_analyzer/** - Production package with dual interfaces
```
dfs_analyzer/
├── core/                    # Core algorithms
│   ├── graphs.py           # Graph abstractions (Hypercube, GeneralizedPetersen, ErdosRenyiGraph)
│   ├── rdfs.py             # RDFS algorithm (auto-detects graph-tool)
│   ├── rdfs_graphtool.py   # High-performance backend with multiprocessing (NEW v0.4.0)
│   ├── statistics.py       # Statistical analysis
│   ├── neighbor_analysis.py    # Immediate neighbor analysis
│   ├── opposite_analysis.py    # Opposite vertex analysis (hypercube)
│   ├── custom_vertex_analysis.py  # Custom vertex pair analysis
│   └── gnp_graph.py        # G(n,p) random graph implementation
├── experiments/             # Experiment management
│   ├── config.py           # ExperimentConfig (multi-graph support)
│   ├── runner.py           # ExperimentRunner (supports num_processes parameter)
│   ├── results.py          # Results storage/export
│   ├── neighbor_runner.py       # Neighbor analysis runner
│   ├── opposite_runner.py       # Opposite vertex runner
│   ├── custom_vertex_runner.py  # Custom vertex pair runner
│   └── gnp_batch_runner.py      # G(n,p) batch experiment runner
└── ui/                     # User interfaces
    ├── cli.py              # Interactive CLI (all features)
    └── streamlit_app.py    # Web GUI (HPC-compatible)
```

### Key Components

1. **Graph Layer** (`core/graphs.py`, `core/gnp_graph.py`)
   - Abstract `Graph[Vertex]` base class with generic TypeVar
   - **Hypercube(d)** - d-dimensional hypercube graphs
     - Vertices: 2^d binary tuples (e.g., (0,1,0))
     - Edges: Hamming distance 1 (differ in one bit)
     - Degree: d (grows with dimension)
   - **GeneralizedPetersen(n, k)** - GP(n,k) graphs
     - Vertices: 2n (two n-vertex rings as ('outer', i) or ('inner', i))
     - Inner ring connected to outer ring
     - Outer ring has skip-k connections
     - Degree: 3 (constant)
   - **ErdosRenyiGraph(n, p)** - G(n,p) random graphs
     - Vertices: n (labeled 0 to n-1)
     - Each edge appears with probability p
     - NOT regular or symmetric (exploratory analysis)
     - Connectivity checking with BFS
     - Rejection sampling for connected graphs
   - Extensible to additional graph types

2. **Algorithm Layer** (`core/rdfs.py`, `core/rdfs_graphtool.py`)
   - `rdfs()` - Randomized DFS with stack-based traversal
   - Randomizes neighbor order via `rng.shuffle()`
   - Tracks discovery numbers (visit order)
   - `collect_statistics()` - Runs RDFS multiple times
     - Auto-detects graph-tool availability
     - Falls back to pure Python if needed
   - `get_summary_stats()` - Computes mean, variance, etc.
   - Seeded RNG (1832479182) for reproducibility

   **High-Performance Backend** (`core/rdfs_graphtool.py`) - NEW v0.4.0
   - Optional C++ backend via graph-tool library
   - 40x speedup for single-core execution
   - **Multiprocessing support for ALL graph types:**
     - Hypercube: Optimized integer arithmetic path
     - Petersen/GNP/Custom: Generic graph-tool path
   - Automatic CPU detection and work distribution
   - Near-linear scaling (70-90% efficiency)
   - Maintains reproducibility with proper RNG seeding
   - Performance: 10-56x speedup on HPC (16-64 cores)

3. **Statistical Analysis** (`core/statistics.py`)
   - `validate_conjecture()` - Tests (n-1)/2 hypothesis
   - `compute_overall_average()` - Mean across all vertices
   - `format_validation_result()` - Human-readable output

4. **Advanced Analysis Modules** (new in v0.3.0)
   - **Neighbor Analysis** (`core/neighbor_analysis.py`)
     - Focuses on immediate neighbors of start vertex
     - Computes discovery ratios (< 1.0 = earlier than average)
     - Uses RDFS sampling
   - **Opposite Vertex** (`core/opposite_analysis.py`)
     - Analyzes diagonally opposite vertex in hypercubes
     - Computes Hamming distance automatically
     - Maximum distance vertex analysis
   - **Custom Vertex Pair** (`core/custom_vertex_analysis.py`)
     - User-specified start and target vertices
     - Works with hypercubes and Petersen graphs
     - Discovery ratio interpretation

5. **Experiment Management** (`experiments/`)
   - `ExperimentConfig` - Configuration for experiments
     - Supports storage-efficient output (summary.txt only by default)
     - Optional: CSV, detailed stats, plots
     - Validates parameters based on graph type
   - **Experiment Runners**:
     - `ExperimentRunner` - Full graph analysis
     - `NeighborAnalysisRunner` - Neighbor focus
     - `OppositeAnalysisRunner` - Opposite vertex (hypercube only)
     - `CustomVertexRunner` - Custom vertex pairs
     - `GNPBatchRunner` - Batch G(n,p) experiments
   - `ExperimentResults` - Exports to CSV, JSON, TXT, Pickle

6. **User Interfaces** (`ui/`)
   - **CLI** (`ui/cli.py`)
     - Interactive menu-driven interface
     - 5 analysis types: Full, Neighbors, Opposite, Custom, G(n,p) Batch
     - 3 graph types: Hypercube, Petersen, G(n,p)
     - Vertex input with validation
     - Progress bars for long experiments
     - Storage-efficient output options
     - Uses RDFS sampling throughout
   - **Web GUI** (`ui/streamlit_app.py`) - NEW!
     - Streamlit-based web interface
     - HPC-compatible via SSH port forwarding
     - All CLI features in interactive UI
     - Real-time progress tracking
     - Sidebar configuration
     - Built-in documentation panel

### Data Flow

```
User Input (CLI) → Graph Type Selection → ExperimentConfig
    ↓
ExperimentRunner → Graph Creation (Hypercube/Petersen)
    ↓
collect_statistics() → rdfs() × N samples (with proportional scaling)
    ↓
Discovery numbers → get_summary_stats() → Statistical analysis
    ↓
validate_conjecture() → ExperimentResults → Export (CSV/JSON/TXT/Pickle)
    ↓
Visualizations (PNG) + Summary files
```

## Code Style Guidelines

### Comment Style (Updated November 2025)
All code uses **imperative style** comments:

```python
# Creates dictionary to store discovery numbers
dist_stats = defaultdict(list)

# Validates dimension is positive
if d < 1:
    raise ValueError(...)

# Iterates through each dimension
for i in range(self.d):
    # Flips bit at position i
    neighbor_list[i] = 1 - neighbor_list[i]
```

**Principles:**
- Use imperative verbs: Creates, Defines, Validates, Returns, Computes
- Be concise but explanatory
- Comment before complex logic
- Every significant code block explained

### Number Formatting
- Display numbers WITHOUT commas: `10000` not `10,000`
- Prevents user confusion when inputting values
- Consistent across all displays and prompts

### Sample Size Recommendations
Uses **proportional scaling** to maintain consistent sampling ratios:

```python
def get_recommended_samples(graph_type, dimension, num_vertices):
    if graph_type == "hypercube":
        # ~3000 samples per vertex (accounts for exponential growth)
        samples_per_vertex = 3000 if dimension > 3 else 2500
        return samples_per_vertex * num_vertices
    else:  # petersen
        # ~1000 samples per vertex (constant degree 3)
        return 1000 * num_vertices
```

**Examples:**
- Hypercube 3D (8 vertices): 20,000 samples
- Hypercube 5D (32 vertices): 96,000 samples
- Petersen GP(5,2) (10 vertices): 10,000 samples
- Petersen GP(10,2) (20 vertices): 20,000 samples

## Development History

### Version 0.5.0 (December 17, 2025)
**Advanced visualization with histogram and layer analysis**

**Major Features:**
1. **Histogram Visualization**
   - Groups vertices into buckets by average discovery number
   - Integer bins centered on whole numbers (e.g., 12.5-13.5 bucket)
   - Dual reference lines: Expected (n-1)/2 (red) and Actual mean (green)
   - Shows distribution pattern across the graph
   - Works for all graph types (Hypercube, Petersen, G(n,p))
   - Created `_create_histogram_plot()` in `experiments/results.py`

2. **Layer Analysis Visualization**
   - **BFS-based distance computation** from starting vertex
   - Groups vertices by graph-theoretic distance (layers)
   - **Dual-plot design:**
     - **Top plot**: Line graph of average discovery number by layer
       - Error bars showing standard deviation within each layer
       - Red reference line for expected (n-1)/2
       - Value annotations for each layer's mean
     - **Bottom plot**: Bar chart showing vertex count per layer
     - **Stats box**: Diameter, average slope (steepness metric)
   - Reveals how DFS discovery order correlates with distance from origin
   - **Slope interpretation:**
     - Flat slope: Discovery order independent of distance
     - Steep slope: Closer vertices discovered much earlier
   - Works universally for all graph types
   - Created `_create_layer_analysis_plot()` in `experiments/results.py`

3. **Automatic Visualization Generation**
   - Updated `_save_plot()` to generate three visualizations:
     1. `visualization.png` - Traditional per-vertex bar chart
     2. `histogram.png` - Distribution by discovery number buckets (NEW)
     3. `layer_analysis.png` - Layer-by-layer distance analysis (NEW)
   - All three generated automatically when `save_plots=True`
   - Controlled by `ExperimentConfig.save_plots` flag

**Testing:**
- Created `test_histogram.py` - Hypercube histogram test
- Created `test_histogram_petersen.py` - Petersen histogram test
- Created `test_histogram_gnp.py` - G(n,p) histogram test
- Created `test_layer_analysis.py` - Hypercube layer analysis
- Created `test_gnp_layers.py` - G(n,p) layer analysis test
- Updated all test scripts to enable `save_plots=True`

**Results Validation:**
- **Hypercube 6D (64 vertices):**
  - Histogram: Origin at 0, others clustered 28-33 (correct - DFS randomness)
  - Layer analysis: 7 layers (0-6), steep initial rise then plateau
- **Petersen GP(10,3) (20 vertices):**
  - Histogram: Symmetric distribution around expected value
  - Layer analysis: 6 layers (0-5), average slope 2.16/layer
- **G(n,p) n=30, p=0.3 (30 vertices):**
  - Histogram: Tight distribution around expected value
  - Layer analysis: 4 layers (0-3, small diameter), steep slope 5.58/layer

**Implementation Details:**
- BFS algorithm computes shortest path distances from origin
- `collections.deque` for efficient queue operations
- Layer grouping: `distances[vertex] -> layer_number`
- Statistical aggregation: mean, std dev per layer
- Matplotlib subplots with shared x-axis for dual plots
- Automatic diameter detection (max distance found)
- Slope calculation: average change between consecutive layers

**Updated Files:**
- `dfs_analyzer/experiments/results.py` - Added `_create_histogram_plot()` and `_create_layer_analysis_plot()`
- `test_histogram.py` - Hypercube histogram test
- `test_histogram_petersen.py` - Updated with plots enabled
- `test_histogram_gnp.py` - Fixed validation errors
- `test_layer_analysis.py` - Comprehensive layer analysis test
- `test_gnp_layers.py` - NEW: G(n,p) layer analysis

**Key Statistics:**
- 3 visualizations generated per experiment
- BFS layer computation: O(V + E) time complexity
- Works for all graph types universally
- Provides geometric insight into DFS behavior
- Validates (n-1)/2 conjecture across distance layers

### Version 0.4.0 (December 16, 2025)
**HPC-ready with graph-tool integration and multiprocessing**

**Major Features:**
1. **graph-tool Integration**
   - High-performance C++ backend via graph-tool library
   - Created `dfs_analyzer/core/rdfs_graphtool.py` (500+ lines)
   - Automatic detection and fallback to pure Python
   - 40x speedup for single-core execution vs pure Python
   - Installation via conda: `conda install -c conda-forge graph-tool`
   - Created `environment.yml` for conda environment
   - Created `INSTALLATION.md` with platform-specific instructions

2. **Multiprocessing Support for ALL Graph Types**
   - **Hypercube optimized path:**
     - `_run_rdfs_batch_hypercube()` worker function
     - Pure integer arithmetic (XOR bit flipping)
     - Numpy arrays for visited tracking
     - Pre-computed neighbor masks
     - 34-56x speedup on 32 cores
   - **Generic graph path (Petersen, GNP, custom):**
     - `_run_rdfs_batch_generic()` worker function
     - graph-tool backend for neighbor lookups
     - Works for any graph type
     - 22-23x speedup on 32 cores
   - Automatic CPU detection (uses all available cores)
   - Manual control via `num_processes` parameter
   - Near-linear scaling (70-90% efficiency)
   - Maintains reproducibility with seeded RNG offsets

3. **ExperimentRunner Integration**
   - Updated `experiments/runner.py` with `num_processes` parameter
   - Passes through to `collect_statistics()`
   - Zero code changes needed for existing code
   - Works automatically when graph-tool installed

4. **HPC Support**
   - Created `hpc_job_template.slurm` - Complete SLURM job script
   - Created `HPC_GUIDE.md` - Comprehensive HPC documentation
   - Resource sizing recommendations (CPUs, memory, time)
   - Platform-specific instructions (Linux, macOS, Windows/WSL, HPC clusters)
   - SSH port forwarding for GUI access from HPC
   - Job arrays for parameter sweeps
   - Monitoring and troubleshooting sections

5. **Testing and Documentation**
   - Created `test_parallel.py` - Hypercube multiprocessing test
   - Created `test_parallel_petersen.py` - Petersen multiprocessing test
   - Created `HPC_READY_SUMMARY.md` - Quick reference
   - Created `MULTIPROCESSING_UPDATE.md` - Technical details
   - Updated `HPC_GUIDE.md` with all graph types

**Performance Benchmarks:**
- Hypercube 15D, 100k samples:
  - Laptop single: 2.8 hours
  - HPC 16 cores: 5-10 minutes (17-34x)
  - HPC 32 cores: 3-5 minutes (34-56x)
- Petersen GP(100,10), 200k samples:
  - Laptop single: 45 minutes
  - HPC 16 cores: 3-4 minutes (11-15x)
  - HPC 32 cores: 2 minutes (22-23x)

**Updated Files:**
- `dfs_analyzer/core/rdfs.py` - Auto-detects graph-tool
- `dfs_analyzer/core/rdfs_graphtool.py` - NEW: High-performance backend
- `dfs_analyzer/experiments/runner.py` - Added num_processes parameter
- `environment.yml` - Fixed pip section for conda installation
- `HPC_GUIDE.md` - Updated with multiprocessing info

**New Files:**
- `test_parallel.py` - Hypercube multiprocessing test
- `test_parallel_petersen.py` - Petersen multiprocessing test
- `hpc_job_template.slurm` - SLURM job script template
- `HPC_READY_SUMMARY.md` - Quick start guide
- `MULTIPROCESSING_UPDATE.md` - Technical documentation

**Key Statistics:**
- 40x single-core speedup with graph-tool
- 10-56x multiprocessing speedup on HPC
- All graph types support multiprocessing
- Automatic CPU detection and scaling
- Production-ready for large-scale research

### Version 0.3.0 (December 5, 2025)
**Hybrid GUI/CLI system with advanced analysis modes**

**Major Features:**
1. **Web GUI with Streamlit**
   - Created `dfs_analyzer/ui/streamlit_app.py` (500+ lines)
   - HPC-compatible via SSH port forwarding
   - All analysis types in interactive interface
   - Real-time progress bars and validation
   - Sidebar configuration with collapsible sections
   - Built-in documentation and help panel
   - Created `run_gui.py` launcher with Windows compatibility fix
   - Created `GUI_README.md` comprehensive documentation

2. **Advanced Analysis Modes**
   - **Storage-Efficient Output**
     - Only summary.txt generated by default
     - Optional flags for CSV, detailed stats, plots
     - Prevents storage clogging on large experiments
   - **Immediate Neighbor Analysis**
     - Focus on vertices adjacent to start
     - Discovery ratio analysis (< 1.0 = earlier)
     - Uses RDFS sampling
     - Created `core/neighbor_analysis.py`
   - **Opposite Vertex Analysis**
     - Diagonally opposite vertex in hypercubes
     - Maximum Hamming distance analysis
     - Created `core/opposite_analysis.py`
   - **Custom Vertex Pair Analysis**
     - User-specified start and target vertices
     - Works with hypercubes and Petersen graphs
     - Binary tuple input for hypercubes
     - Ring/index input for Petersen
     - Automatic Hamming distance computation
     - Created `core/custom_vertex_analysis.py`

3. **G(n,p) Random Graph Support**
   - Erdős-Rényi random graphs
   - Batch mode for multiple graphs
   - Connectivity threshold warnings
   - Rejection sampling for connected graphs
   - Aggregate statistics across graphs
   - Created `core/gnp_graph.py`
   - Created `experiments/gnp_batch_runner.py`

**Experiment Runners Created:**
- `experiments/random_walk_runner.py` - Laplacian analysis
- `experiments/neighbor_runner.py` - Neighbor analysis
- `experiments/opposite_runner.py` - Opposite vertex
- `experiments/custom_vertex_runner.py` - Custom pairs
- `experiments/gnp_batch_runner.py` - G(n,p) batch

**Test Scripts Created:**
- `test_laplacian.py` - Laplacian method validation
- `test_comparison.py` - RDFS vs Laplacian comparison
- `test_neighbors.py` - Neighbor analysis
- `test_opposite.py` - Opposite vertex
- `test_custom_vertex.py` - Custom vertex pairs
- `test_gnp.py` - G(n,p) batch experiments

**Updated Files:**
- `requirements.txt` - Added streamlit>=1.28.0, networkx>=3.0
- `dfs_analyzer/ui/cli.py` - Expanded to support all new features
- `dfs_analyzer/experiments/config.py` - Storage-efficient flags
- `dfs_analyzer/experiments/results.py` - Conditional file saving

**Key Statistics:**
- 5 analysis types available
- 2 analysis methods (RDFS, Laplacian)
- 3 graph types supported
- All tests passing with 0.0000% error
- HPC-compatible GUI and CLI

### Version 0.2.0 (November 16, 2025)
**Multi-graph support with proportional sampling**
- Added `GeneralizedPetersen(n, k)` graph class to core/graphs.py
- Updated `ExperimentConfig` to support `petersen_k` parameter
- Updated `ExperimentRunner` to create Petersen graphs
- Updated CLI with graph type selection menu (Hypercube vs Petersen)
- Implemented proportional sample scaling based on graph complexity
  - Hypercubes: ~3000 samples per vertex
  - Petersen: ~1000 samples per vertex (constant degree)
- Created test scripts: `test_hypercube.py`, `test_petersen.py`
- All tests passing with 0.0000% error

### Version 0.1.1 (November 15, 2025)
**Code style and compatibility updates**
- Updated author name to Venkat Mahesh Mandava
- Removed comma formatting from all number displays
- Updated all code comments to imperative style
- Added comprehensive inline comments throughout core modules
- Fixed Windows UTF-8 encoding issues
- Set up GitHub repository with proper .gitignore

### Version 0.1.0 (November 7, 2025)
**Initial production release**
- Refactored research code into proper package structure
- Created interactive menu-driven CLI
- Added multiple export formats (CSV, JSON, TXT, Pickle)
- Comprehensive documentation
- Hypercube graph support only

### Pre-1.0 Research Phase
- Original research using Jupyter notebooks
- Separate script files (mygraphs.py, myrdfs.py, etc.)
- Various graph type experiments (Petersen, G(n,p), grids)
- Files now archived (not in production repo)

## Interface Usage

### GUI vs CLI Decision Matrix

| Use Case | GUI | CLI | Batch |
|----------|-----|-----|-------|
| Learning the tool | ✅ Best | ⚠️ OK | ❌ No |
| Parameter exploration | ✅ Best | ⚠️ Manual | ❌ No |
| Small experiments (<10k samples) | ✅ Best | ✅ Good | ⚠️ OK |
| Large experiments (>100k samples) | ⚠️ Limited | ✅ Good | ✅ Best |
| Automation/scripts | ❌ No | ✅ Good | ✅ Best |
| HPC batch jobs | ❌ No | ✅ Good | ✅ Best |
| HPC interactive | ✅ Yes* | ✅ Yes | ❌ No |
| Visual feedback | ✅ Best | ⚠️ Text | ❌ None |

*Via SSH port forwarding

### GUI Usage (Streamlit)

**Local Usage:**
```bash
# Install dependencies (first time)
pip install -r requirements.txt

# Launch GUI
python run_gui.py

# Browser opens to: http://localhost:8501
```

**HPC Usage:**
```bash
# Terminal 1: SSH with port forwarding
ssh -L 8501:localhost:8501 user@hpc.edu

# Load modules
module load python/3.10

# Install dependencies (first time only)
pip install --user -r requirements.txt

# Launch GUI
python run_gui.py

# Terminal 2 / Local browser:
# Navigate to: http://localhost:8501
```

**Windows PATH Issue Fix:**
If `streamlit` command not recognized, the launcher script uses `python -m streamlit` automatically.

**GUI Features:**
- 5 analysis types (dropdown selection)
- 2 methods (RDFS/Laplacian radio buttons)
- 3 graph types (Hypercube, Petersen, G(n,p))
- Interactive parameter sliders
- Vertex input with validation
- Real-time progress bars
- Results display in-browser
- Sidebar configuration
- Built-in help panel

## Common Tasks

### HPC Usage (NEW v0.4.0)

**Quick Start:**
```bash
# 1. Upload code to HPC
scp -r dfs-analyzer/ username@hpc.edu:~/

# 2. Setup on HPC
ssh username@hpc.edu
cd dfs-analyzer
conda env create -f environment.yml
conda activate dfs-analyzer

# 3. Verify graph-tool
python -c "from dfs_analyzer.core.rdfs_graphtool import is_available; print('graph-tool:', is_available())"

# 4. Customize job script
cp hpc_job_template.slurm my_job.slurm
nano my_job.slurm  # Edit DIMENSION, NUM_SAMPLES, --cpus-per-task

# 5. Submit job
sbatch my_job.slurm

# 6. Monitor
squeue -u $USER
tail -f dfs-*.out
```

**Key SLURM Parameters:**
```bash
#SBATCH --cpus-per-task=32    # Number of cores (enables multiprocessing)
#SBATCH --mem=64G             # Memory
#SBATCH --time=2:00:00        # Time limit
```

**Multiprocessing Control:**
```python
# Automatic (uses all allocated CPUs)
runner = ExperimentRunner()
results = runner.run(config)

# Manual control
results = runner.run(config, num_processes=32)

# Disable multiprocessing
results = runner.run(config, num_processes=1)
```

**Resource Sizing:**
| Dimension | CPUs | Memory | Time | Expected Runtime |
|-----------|------|--------|------|------------------|
| 10D       | 16   | 8G     | 30min | ~2 min |
| 12D       | 16   | 16G    | 1hr | ~8 min |
| 15D       | 32   | 64G    | 1hr | ~18 min |
| 18D       | 32   | 256G   | 4hr | ~1.5 hr |
| 20D       | 64   | 512G   | 8hr | ~3 hr |

See `HPC_GUIDE.md` for complete documentation.

### Running Experiments (CLI)
```bash
# Start interactive CLI
python3 run_analyzer.py

# Then follow prompts:
# 1. Select graph type (Hypercube or Petersen)
# 2. Enter graph parameters
#    - Hypercube: dimension d (creates 2^d vertices)
#    - Petersen: n and k (creates GP(n,k) with 2n vertices)
# 3. Choose sample size (or use recommended value)
# 4. Run experiment
# 5. View results in data_output/

# Quick tests
python3 test_hypercube.py   # Test 5D hypercube with 500 samples
python3 test_petersen.py    # Test GP(5,2) with 500 samples
```

### Extending with New Graph Types
```python
# In dfs_analyzer/core/graphs.py
class NewGraph(Graph[YourVertexType]):
    def get_start_vertex(self) -> YourVertexType:
        # Returns starting vertex

    def get_adj_list(self, v: YourVertexType) -> list[YourVertexType]:
        # Returns neighbors

    def number_vertices(self) -> int:
        # Returns vertex count

    # Implement other abstract methods...
```

### Adding to ExperimentRunner
```python
# In dfs_analyzer/experiments/runner.py
def _create_graph(self, config):
    if config.graph_type == "hypercube":
        return Hypercube(config.dimension)
    elif config.graph_type == "petersen":
        return GeneralizedPetersen(config.dimension, config.petersen_k)
    elif config.graph_type == "newgraph":
        return NewGraph(config.dimension)
    # Add new graph types here
```

### Real-World Examples

**Hypercube Experiments:**
```python
config = ExperimentConfig(
    graph_type="hypercube",
    dimension=5,         # 32 vertices
    num_samples=96000,   # Proportionally scaled
    rng_seed=1832479182
)
```

**Petersen Graph Experiments:**
```python
config = ExperimentConfig(
    graph_type="petersen",
    dimension=5,         # n = 5
    petersen_k=2,        # k = 2 (classic Petersen graph)
    num_samples=10000,   # Proportionally scaled
    rng_seed=1832479182
)
```

## Output Files

Every experiment generates (when respective flags enabled):
```
data_output/experiment-name-timestamp/
├── summary.txt              # Human-readable summary (always generated)
├── data.csv                 # Per-vertex statistics (Excel) [if save_csv=True]
├── data.json                # Machine-readable format [if export_formats includes 'json']
├── detailed_stats.txt       # Full statistical report [if save_detailed_stats=True]
├── visualization.png        # Bar chart with (n-1)/2 line [if save_plots=True]
├── histogram.png            # Distribution by discovery number buckets [if save_plots=True] (NEW v0.5.0)
├── layer_analysis.png       # Distance layer analysis [if save_plots=True] (NEW v0.5.0)
└── data.pickle              # Raw data for reanalysis [if export_formats includes 'pickle']
```

**Visualization Details (v0.5.0):**
1. **visualization.png** - Traditional per-vertex bar chart
   - X-axis: Individual vertices (sorted)
   - Y-axis: Average discovery number
   - Red line: Expected (n-1)/2
   - Shows per-vertex variation

2. **histogram.png** - Distribution pattern
   - X-axis: Discovery number buckets (integer bins)
   - Y-axis: Number of vertices in each bucket
   - Red line: Expected (n-1)/2
   - Green line: Actual mean
   - Shows how vertices cluster around expected value

3. **layer_analysis.png** - Distance-based analysis
   - **Top plot**: Average discovery number by distance layer
     - Line graph with error bars (std deviation)
     - Red line: Expected (n-1)/2
     - Value annotations on each point
   - **Bottom plot**: Vertex count per layer
     - Bar chart showing layer sizes
   - **Stats box**: Diameter and average slope
   - Shows correlation between distance and discovery order

## Testing

Manual testing with provided test scripts:
```bash
# Test hypercube implementation
python3 test_hypercube.py
# Expected: Hypercube 5D (32 vertices), 500 samples, 0.0000% error

# Test Petersen graph implementation
python3 test_petersen.py
# Expected: Petersen GP(5,2) (10 vertices), 500 samples, 0.0000% error

# Test imports
python3 -c "from dfs_analyzer.core import Hypercube, GeneralizedPetersen; print('✓ Success')"

# Interactive testing
python3 run_analyzer.py
# Choose graph type, enter parameters, verify results
```

## Documentation

User-facing documentation:
- `README.md` - Main documentation
- `QUICKSTART.md` - Quick start guide
- `WINDOWS_NOTES.md` - Windows-specific help
- `GUI_README.md` - Web GUI documentation and HPC usage
- `CHANGELOG.md` - Version history

Internal documentation (not in repo):
- `CLAUDE.md` - This file (development guidance)
- `CODE_STYLE_UPDATE.md` - Comment style changes
- `PROJECT_STATUS.md` - Development status
- `HPC_GUIDE.md` - HPC deployment guide
- `DIMENSION_LIMITS.md` - Graph size limits

## Key Principles

1. **Modularity** - Core, experiments, UI clearly separated
2. **Extensibility** - Easy to add new graph types
3. **Reproducibility** - Seeded RNG, saved configs
4. **User-Friendly** - Interactive CLI with validation
5. **Professional** - Clean code, good comments, documentation

## Future Enhancements

Potential additions:
- [ ] Additional symmetric regular graphs:
  - [ ] Complete graphs K_n
  - [ ] Cycle graphs C_n
  - [ ] Complete bipartite K_{n,n}
  - [ ] Johnson graphs J(n,k)
  - [ ] Kneser graphs K(n,k)
  - [ ] Hamming graphs H(d,q)
  - [ ] Cayley graphs
  - [ ] Grid graphs (2D, 3D)
- [ ] GUI enhancements:
  - [ ] Matplotlib/Plotly visualizations in GUI
  - [ ] Download buttons for results
  - [ ] Experiment history and comparison
  - [ ] Parameter presets/templates
- [ ] PyPI package distribution
- [ ] Automated test suite with pytest
- [ ] ~~Performance optimizations (Cython, numba)~~ - **DONE via graph-tool (C++ backend)**
- [ ] Docker container for HPC deployment
- [ ] GPU acceleration for massive graphs (20D+)
- [ ] Distributed computing across multiple HPC nodes

## Completed Enhancements
- [✓] Generalized Petersen graphs GP(n,k)
- [✓] Proportional sample size scaling
- [✓] Multi-graph CLI interface
- [✓] Graph-specific parameter validation
- [✓] Streamlit web GUI (HPC-compatible)
- [✓] Erdős-Rényi G(n,p) random graphs
- [✓] Batch processing for G(n,p) graphs
- [✓] Laplacian-based random walk analysis
- [✓] Storage-efficient output options
- [✓] Immediate neighbor analysis
- [✓] Opposite vertex analysis (hypercube)
- [✓] Custom vertex pair analysis
- [✓] Dual interface system (GUI + CLI)
- [✓] **graph-tool integration (40x speedup)** - v0.4.0
- [✓] **Multiprocessing for all graph types (10-56x speedup on HPC)** - v0.4.0
- [✓] **HPC support with SLURM templates** - v0.4.0
- [✓] **Automatic CPU detection and scaling** - v0.4.0
- [✓] **Conda environment with all dependencies** - v0.4.0
- [✓] **Histogram visualization (distribution by discovery buckets)** - v0.5.0
- [✓] **Layer analysis visualization (distance-based DFS patterns)** - v0.5.0
- [✓] **BFS-based distance computation** - v0.5.0
- [✓] **Triple visualization system (bar/histogram/layer)** - v0.5.0

## Repository

GitHub: https://github.com/theking7415/capstone-project-dfs-analyzer

Clean production repository contains:
- ✅ `dfs_analyzer/` package
- ✅ `run_analyzer.py` launcher
- ✅ User documentation (README, QUICKSTART, etc.)
- ✅ `requirements.txt`
- ❌ No research scripts or notebooks
- ❌ No internal development docs
- ❌ No test files or data files

## Contact

Author: Venkat Mahesh Mandava
Institution: Ashoka University
Project: Capstone 2025
